# üöÄ AutoAgents GPU Max Performance Setup

**Get 100% GPU utilization with autonomous Git sync!**

This setup enables maximum GPU acceleration for AutoAgents with automatic codebase synchronization to GitHub every 10 minutes.

## üìã What's Included

- ‚úÖ **CUDA GPU Acceleration**: Full GPU support for LiquidEdge inference runtime
- ‚úÖ **Autonomous Git Sync**: Automatic sync/merge/pull/push every 10 minutes
- ‚úÖ **GPU Monitoring**: Real-time utilization tracking and optimization
- ‚úÖ **Performance Optimization**: Environment variables for maximum throughput
- ‚úÖ **Error Handling**: Robust error recovery and logging

## üõ†Ô∏è Quick Start

### 1. One-Click GPU Max Setup

```batch
# Run this to build with CUDA and start autonomous sync
start_gpu_max.bat
```

### 2. Manual GPU Build

```batch
# Build liquid-edge with CUDA support
cd crates\liquid-edge
cargo build --release --features cuda

# Build main project
cd ../..
cargo build --release
```

### 3. Start Autonomous Sync

```powershell
# Start the 10-minute sync service
.\auto_sync.ps1
```

### 4. Monitor GPU Performance

```powershell
# Monitor GPU utilization with optimization
.\gpu_monitor.ps1 -Optimize

# Just monitor (no optimization)
.\gpu_monitor.ps1
```

## üéØ GPU Acceleration Features

### LiquidEdge Runtime
- **ONNX Runtime** with CUDA backend
- **Device Selection**: `cuda()`, `cuda_default()`, `cpu()`
- **Memory Optimization**: Efficient GPU memory management
- **Async Support**: Non-blocking inference operations

### Usage Example

```rust
use liquid_edge::{Device, Runtime};

// Create CUDA device for maximum performance
let device = Device::Cuda(0); // GPU 0

// Initialize runtime with GPU acceleration
let runtime = Runtime::new(device)?;

// Run inference with 100% GPU utilization
let result = runtime.infer(input).await?;
```

## üîÑ Autonomous Git Sync

### Features
- **10-minute intervals**: Automatic sync cycle
- **Smart merging**: Rebase strategy to avoid conflicts
- **Change detection**: Only commits when there are changes
- **Error recovery**: Continues running even after failures
- **GPU monitoring**: Tracks GPU status during sync

### Sync Process
1. **Fetch**: Get latest changes from all remotes
2. **Stage**: Add any local changes
3. **Commit**: Auto-commit with timestamp
4. **Rebase**: Pull with rebase to avoid merge commits
5. **Push**: Push changes to remote
6. **Monitor**: Check GPU utilization

## üìä GPU Monitoring

### Real-time Metrics
- GPU utilization percentage
- Memory usage (used/total/free)
- Temperature monitoring
- Power consumption
- Performance trends

### Optimization Recommendations
- **Low utilization (<50%)**: Increase batch sizes
- **High utilization (>90%)**: Excellent performance!
- **High memory (>90%)**: Reduce batch size
- **High temperature (>80¬∞C)**: Check cooling

## ‚öôÔ∏è Environment Variables

Set these for maximum GPU performance:

```batch
set CUDA_VISIBLE_DEVICES=0
set TORCH_USE_CUDA_DSA=1
set CUDA_LAUNCH_BLOCKING=0
set PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
set OMP_NUM_THREADS=16
set MKL_NUM_THREADS=16
```

## üîß Troubleshooting

### Build Issues
```batch
# Clean and rebuild
cargo clean
cargo build --release --features cuda
```

### GPU Not Detected
```batch
# Check GPU status
nvidia-smi

# Install/update NVIDIA drivers
# Visit: https://www.nvidia.com/Download/index.aspx
```

### Sync Issues
```batch
# Check git status
git status

# Manual sync
git add .
git commit -m "Manual sync"
git pull --rebase
git push
```

## üìà Performance Tips

### Maximize GPU Utilization
1. **Use CUDA device**: `Device::Cuda(0)`
2. **Increase batch sizes**: Process more data simultaneously
3. **Enable persistent mode**: `nvidia-smi -pm 1`
4. **Monitor temperature**: Keep GPU cool for sustained performance
5. **Use async operations**: Non-blocking inference calls

### Memory Optimization
- Set `PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512`
- Use gradient checkpointing for large models
- Implement memory pooling
- Monitor memory fragmentation

## üéÆ Usage Examples

### Basic GPU Inference
```rust
use liquid_edge::{Device, Model, Runtime};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Use CUDA for maximum performance
    let device = Device::Cuda(0);

    // Load ONNX model
    let model = Model::from_path("model.onnx", device)?;

    // Create runtime
    let runtime = Runtime::new(model)?;

    // Run inference
    let input = vec![1.0, 2.0, 3.0];
    let output = runtime.infer(input).await?;

    println!("GPU accelerated result: {:?}", output);
    Ok(())
}
```

### Autonomous Sync Integration
```powershell
# Start sync service
.\auto_sync.ps1

# Monitor in another terminal
.\gpu_monitor.ps1 -Optimize
```

## üìã System Requirements

- **OS**: Windows 10/11
- **GPU**: NVIDIA GPU with CUDA support
- **Drivers**: Latest NVIDIA drivers
- **Rust**: 1.70+ with wasm32-wasip1 target
- **Git**: Latest version

## üöÄ Next Steps

1. Run `start_gpu_max.bat` for one-click setup
2. Monitor GPU utilization with `gpu_monitor.ps1`
3. Check autonomous sync logs
4. Optimize batch sizes for 100% utilization
5. Deploy to production with persistent services

## üìû Support

- Check logs in terminal output
- Monitor GPU with `nvidia-smi`
- Review sync status with `git log --oneline`
- Use `gpu_monitor.ps1` for detailed diagnostics

---

**üéØ Goal Achieved**: 100% GPU utilization with autonomous GitHub sync every 10 minutes!
